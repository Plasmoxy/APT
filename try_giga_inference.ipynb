{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/petrik/miniforge3/envs/apt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-26 10:30:20,419] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/petrik/miniforge3/envs/apt/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "import sys\n",
    "import logging\n",
    "import transformers\n",
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "import nltk\n",
    "import numpy as np\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import (HfArgumentParser, EvalPrediction, DataCollatorForSeq2Seq, set_seed)\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from deepspeed.profiling.flops_profiler import get_model_profile\n",
    "from datasets import load_metric\n",
    "from models.model_args import ModelArguments\n",
    "from utils.utils import *\n",
    "from utils.minus_utils import efficiency_testing, input_constructor, compare_parameters\n",
    "from utils.analysis_utils import gen_run_report\n",
    "from trainer.trainer_seq2seq_minus import MinusSeq2SeqTrainer\n",
    "from args import MinusTrainingArguments, Seq2SeqDataTrainingArguments\n",
    "from loralib.layers import LoRALayer\n",
    "from models import build_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'summary', 'orig_idx'],\n",
      "        num_rows: 3783821\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'summary', 'orig_idx'],\n",
      "        num_rows: 94405\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'summary', 'orig_idx'],\n",
      "        num_rows: 94406\n",
      "    })\n",
      "    test_original: Dataset({\n",
      "        features: ['article', 'summary', 'orig_idx'],\n",
      "        num_rows: 1822\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "\n",
    "dataset_name = \"Plasmoxy/gigatrue\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Remove unnecessary columns if present\n",
    "for split in dataset.keys():\n",
    "    if \"article_len_approx\" in dataset[split].column_names:\n",
    "        dataset[split] = dataset[split].remove_columns(\"article_len_approx\")\n",
    "    if \"summary_len_approx\" in dataset[split].column_names:\n",
    "        dataset[split] = dataset[split].remove_columns(\"summary_len_approx\")\n",
    "\n",
    "# Halved validation set option\n",
    "if True:\n",
    "    # Split 'val' into two halves\n",
    "    val_split = dataset['validation']\n",
    "    half_index = len(val_split) // 2\n",
    "    validation_split = val_split.select(range(half_index))\n",
    "    test_split = val_split.select(range(half_index, len(val_split)))\n",
    "\n",
    "    # Create a new DatasetDict with the updated splits\n",
    "    dataset = DatasetDict({\n",
    "        'train': dataset['train'],\n",
    "        'validation': validation_split,\n",
    "        'test': test_split,\n",
    "        'test_original': dataset['test']\n",
    "    })\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'output/bubi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grafting mask learning rate is set to be the same as mask learning rate.\n"
     ]
    }
   ],
   "source": [
    "model_args = ModelArguments(\n",
    "    model_name_or_path=model_path,\n",
    "    apply_lora=True,\n",
    "    lora_alpha=16,\n",
    "    lora_r=8,  # from script argument lora_r\n",
    "    use_fast_tokenizer=True,\n",
    "    model_revision=\"main\",\n",
    "    use_auth_token=False,\n",
    "    do_auto_pruning=False\n",
    ")\n",
    "\n",
    "data_args = Seq2SeqDataTrainingArguments(\n",
    "    task_name=\"gigatrue\",\n",
    "    max_input_length=110,\n",
    "    max_target_length=35,\n",
    ")\n",
    "\n",
    "training_args = MinusTrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.06,\n",
    "    bf16=True,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    save_strategy=\"no\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_steps=5000,\n",
    "    logging_steps=1000,\n",
    "    \n",
    "    # Minus specific arguments\n",
    "    adapter_type=\"lora\",\n",
    "    minus_scheduler=True,\n",
    "    mac_constraint=0.4,\n",
    "    pruning_scheduler=\"cubic_gradual\",\n",
    "    pruning_scheduler_strategy=\"saliency\",\n",
    "    param_allocation_strategy=\"running_fisher\",\n",
    "    param_resizing_strategy=\"tophalf_limited\",\n",
    "    pruning_frequency=-1,\n",
    "    num_prunings=10,\n",
    "    pruning_batch_size=64,\n",
    "    pruning_batches=8,\n",
    "    pruning_start=-1,\n",
    "    pruning_stop=2,\n",
    "    # pre_pruning_tuning_steps=200,\n",
    "    # sparsity_warmup_epochs=1,\n",
    "    head_scorer_type=\"gradient_l2\",\n",
    "    intermediate_scorer_type=\"gradient_l2\",\n",
    "    pruner_type=\"running_fisher\",\n",
    "    distillation_type=\"self_momentum\",\n",
    "    distill_mapping_strategy=\"dynamic_block_teacher_dynamic_student\",\n",
    "    do_distill=True,\n",
    "    do_virtual_prune=True,\n",
    "    distill_start=-1,\n",
    "    distill_epoch=5,\n",
    "    mask_lr=0.01,\n",
    "    grafting_top_k=-1,\n",
    "    collect_salience=True,\n",
    "    salience_collecting_start=200,\n",
    "    salience_collecting_end=-1,\n",
    "    teacher_param_tuning_config=\"eq:0-5,ev:0-5,dq:0-5,dv:0-5,cq:0-5,cv:0-5,ei:0-5,di:0-5\",\n",
    "    student_param_tuning_config=\"eq:0-5,ev:0-5,dq:0-5,dv:0-5,cq:0-5,cv:0-5,ei:0-5,di:0-5\",\n",
    "    # warmup_param_tuning_config=\"eq:0-5,ev:0-5,dq:0-5,dv:0-5,cq:0-5,cv:0-5,ei:0-5,di:0-5\",\n",
    "    tuning_expanding_ratio=4.0,\n",
    "    max_lora_r=64,  # lora_r * 8\n",
    "    report_to=\"none\",\n",
    "    seed=128\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:  T5Config {\n",
      "  \"_name_or_path\": \"output/bubi\",\n",
      "  \"adap_pruned_heads\": {\n",
      "    \"cross\": {\n",
      "      \"0\": [],\n",
      "      \"1\": [],\n",
      "      \"2\": [],\n",
      "      \"3\": [],\n",
      "      \"4\": [],\n",
      "      \"5\": []\n",
      "    },\n",
      "    \"decoder\": {\n",
      "      \"0\": [],\n",
      "      \"1\": [],\n",
      "      \"2\": [],\n",
      "      \"3\": [],\n",
      "      \"4\": [],\n",
      "      \"5\": []\n",
      "    },\n",
      "    \"encoder\": {\n",
      "      \"0\": [],\n",
      "      \"1\": [],\n",
      "      \"2\": [],\n",
      "      \"3\": [],\n",
      "      \"4\": [],\n",
      "      \"5\": []\n",
      "    }\n",
      "  },\n",
      "  \"apply_lora\": true,\n",
      "  \"architectures\": [\n",
      "    \"AdaPT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"do_distill\": true,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"lora_alpha\": 16,\n",
      "  \"lora_r\": 8,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.33.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "LoRA r using is 8\n",
      "Original config path:  t5-small\n",
      "Pruned heads: {}\n",
      "Deducting masks from the difference between model and weights\n",
      "Encoder layer 0 prune heads: []. Decoder layer 0 prune heads: []. Cross-attention layer 0 prune heads: [].\n",
      "Encoder layer 1 prune heads: []. Decoder layer 1 prune heads: []. Cross-attention layer 1 prune heads: [].\n",
      "Encoder layer 2 prune heads: []. Decoder layer 2 prune heads: []. Cross-attention layer 2 prune heads: [].\n",
      "Encoder layer 3 prune heads: []. Decoder layer 3 prune heads: []. Cross-attention layer 3 prune heads: [].\n",
      "Encoder layer 4 prune heads: []. Decoder layer 4 prune heads: []. Cross-attention layer 4 prune heads: [].\n",
      "Encoder layer 5 prune heads: []. Decoder layer 5 prune heads: []. Cross-attention layer 5 prune heads: [].\n",
      "Encoder:\n",
      "Layer: 0\n",
      "query: torch.Size([512, 512])\n",
      "key: torch.Size([512, 512])\n",
      "value: torch.Size([512, 512])\n",
      "output: torch.Size([512, 512])\n",
      "up: torch.Size([1803, 512])\n",
      "down: torch.Size([512, 1803])\n",
      "Layer: 1\n",
      "query: torch.Size([512, 512])\n",
      "key: torch.Size([512, 512])\n",
      "value: torch.Size([512, 512])\n",
      "output: torch.Size([512, 512])\n",
      "up: torch.Size([1286, 512])\n",
      "down: torch.Size([512, 1286])\n",
      "Layer: 2\n",
      "query: torch.Size([512, 512])\n",
      "key: torch.Size([512, 512])\n",
      "value: torch.Size([512, 512])\n",
      "output: torch.Size([512, 512])\n",
      "up: torch.Size([883, 512])\n",
      "down: torch.Size([512, 883])\n",
      "Layer: 3\n",
      "query: torch.Size([512, 512])\n",
      "key: torch.Size([512, 512])\n",
      "value: torch.Size([512, 512])\n",
      "output: torch.Size([512, 512])\n",
      "up: torch.Size([826, 512])\n",
      "down: torch.Size([512, 826])\n",
      "Layer: 4\n",
      "query: torch.Size([512, 512])\n",
      "key: torch.Size([512, 512])\n",
      "value: torch.Size([512, 512])\n",
      "output: torch.Size([512, 512])\n",
      "up: torch.Size([708, 512])\n",
      "down: torch.Size([512, 708])\n",
      "Layer: 5\n",
      "query: torch.Size([512, 512])\n",
      "key: torch.Size([512, 512])\n",
      "value: torch.Size([512, 512])\n",
      "output: torch.Size([512, 512])\n",
      "up: torch.Size([916, 512])\n",
      "down: torch.Size([512, 916])\n",
      "Decoder:\n",
      "Layer: 0\n",
      "self-attention query: torch.Size([512, 512])\n",
      "self-attention key: torch.Size([512, 512])\n",
      "self-attention value: torch.Size([512, 512])\n",
      "self-attention output: torch.Size([512, 512])\n",
      "cross-attention query: torch.Size([512, 512])\n",
      "cross-attention key: torch.Size([512, 512])\n",
      "cross-attention value: torch.Size([512, 512])\n",
      "cross-attention output: torch.Size([512, 512])\n",
      "up: torch.Size([1271, 512])\n",
      "down: torch.Size([512, 1271])\n",
      "Layer: 1\n",
      "self-attention query: torch.Size([512, 512])\n",
      "self-attention key: torch.Size([512, 512])\n",
      "self-attention value: torch.Size([512, 512])\n",
      "self-attention output: torch.Size([512, 512])\n",
      "cross-attention query: torch.Size([512, 512])\n",
      "cross-attention key: torch.Size([512, 512])\n",
      "cross-attention value: torch.Size([512, 512])\n",
      "cross-attention output: torch.Size([512, 512])\n",
      "up: torch.Size([783, 512])\n",
      "down: torch.Size([512, 783])\n",
      "Layer: 2\n",
      "self-attention query: torch.Size([512, 512])\n",
      "self-attention key: torch.Size([512, 512])\n",
      "self-attention value: torch.Size([512, 512])\n",
      "self-attention output: torch.Size([512, 512])\n",
      "cross-attention query: torch.Size([512, 512])\n",
      "cross-attention key: torch.Size([512, 512])\n",
      "cross-attention value: torch.Size([512, 512])\n",
      "cross-attention output: torch.Size([512, 512])\n",
      "up: torch.Size([856, 512])\n",
      "down: torch.Size([512, 856])\n",
      "Layer: 3\n",
      "self-attention query: torch.Size([512, 512])\n",
      "self-attention key: torch.Size([512, 512])\n",
      "self-attention value: torch.Size([512, 512])\n",
      "self-attention output: torch.Size([512, 512])\n",
      "cross-attention query: torch.Size([512, 512])\n",
      "cross-attention key: torch.Size([512, 512])\n",
      "cross-attention value: torch.Size([512, 512])\n",
      "cross-attention output: torch.Size([512, 512])\n",
      "up: torch.Size([1014, 512])\n",
      "down: torch.Size([512, 1014])\n",
      "Layer: 4\n",
      "self-attention query: torch.Size([512, 512])\n",
      "self-attention key: torch.Size([512, 512])\n",
      "self-attention value: torch.Size([512, 512])\n",
      "self-attention output: torch.Size([512, 512])\n",
      "cross-attention query: torch.Size([512, 512])\n",
      "cross-attention key: torch.Size([512, 512])\n",
      "cross-attention value: torch.Size([512, 512])\n",
      "cross-attention output: torch.Size([512, 512])\n",
      "up: torch.Size([934, 512])\n",
      "down: torch.Size([512, 934])\n",
      "Layer: 5\n",
      "self-attention query: torch.Size([512, 512])\n",
      "self-attention key: torch.Size([512, 512])\n",
      "self-attention value: torch.Size([512, 512])\n",
      "self-attention output: torch.Size([512, 512])\n",
      "cross-attention query: torch.Size([512, 512])\n",
      "cross-attention key: torch.Size([512, 512])\n",
      "cross-attention value: torch.Size([512, 512])\n",
      "cross-attention output: torch.Size([512, 512])\n",
      "up: torch.Size([838, 512])\n",
      "down: torch.Size([512, 838])\n",
      "Layer: 0\n",
      "query: r:  55 , input dim:  512 , output dim:  512\n",
      "key: frozen Linear layer\n",
      "value: r:  29 , input dim:  512 , output dim:  512\n",
      "output: frozen Linear layer\n",
      "up: r:  58 , input dim:  512 , output dim:  1803\n",
      "down: frozen Linear layer\n",
      "Layer: 1\n",
      "query: r:  55 , input dim:  512 , output dim:  512\n",
      "key: frozen Linear layer\n",
      "value: r:  45 , input dim:  512 , output dim:  512\n",
      "output: frozen Linear layer\n",
      "up: r:  26 , input dim:  512 , output dim:  1286\n",
      "down: frozen Linear layer\n",
      "Layer: 2\n",
      "query: r:  64 , input dim:  512 , output dim:  512\n",
      "key: frozen Linear layer\n",
      "value: r:  29 , input dim:  512 , output dim:  512\n",
      "output: frozen Linear layer\n",
      "up: r:  28 , input dim:  512 , output dim:  883\n",
      "down: frozen Linear layer\n",
      "Layer: 3\n",
      "query: r:  64 , input dim:  512 , output dim:  512\n",
      "key: frozen Linear layer\n",
      "value: r:  45 , input dim:  512 , output dim:  512\n",
      "output: frozen Linear layer\n",
      "up: r:  15 , input dim:  512 , output dim:  826\n",
      "down: frozen Linear layer\n",
      "Layer: 4\n",
      "query: r:  64 , input dim:  512 , output dim:  512\n",
      "key: frozen Linear layer\n",
      "value: r:  64 , input dim:  512 , output dim:  512\n",
      "output: frozen Linear layer\n",
      "up: r:  15 , input dim:  512 , output dim:  708\n",
      "down: frozen Linear layer\n",
      "Layer: 5\n",
      "query: r:  64 , input dim:  512 , output dim:  512\n",
      "key: frozen Linear layer\n",
      "value: r:  26 , input dim:  512 , output dim:  512\n",
      "output: frozen Linear layer\n",
      "up: r:  28 , input dim:  512 , output dim:  916\n",
      "down: frozen Linear layer\n",
      "Layer: 0\n",
      "self-query: r:  29 , input dim:  512 , output dim:  512\n",
      "self-key: frozen Linear layer\n",
      "self-value: r:  45 , input dim:  512 , output dim:  512\n",
      "self-output: frozen Linear layer\n",
      "cross-query: r:  64 , input dim:  512 , output dim:  512\n",
      "cross-key: frozen Linear layer\n",
      "cross-value: r:  33 , input dim:  512 , output dim:  512\n",
      "cross-output: frozen Linear layer\n",
      "up: r:  35 , input dim:  512 , output dim:  1271\n",
      "down: frozen Linear layer\n",
      "Layer: 1\n",
      "self-query: r:  27 , input dim:  512 , output dim:  512\n",
      "self-key: frozen Linear layer\n",
      "self-value: r:  17 , input dim:  512 , output dim:  512\n",
      "self-output: frozen Linear layer\n",
      "cross-query: r:  60 , input dim:  512 , output dim:  512\n",
      "cross-key: frozen Linear layer\n",
      "cross-value: r:  17 , input dim:  512 , output dim:  512\n",
      "cross-output: frozen Linear layer\n",
      "up: r:  23 , input dim:  512 , output dim:  783\n",
      "down: frozen Linear layer\n",
      "Layer: 2\n",
      "self-query: r:  38 , input dim:  512 , output dim:  512\n",
      "self-key: frozen Linear layer\n",
      "self-value: r:  17 , input dim:  512 , output dim:  512\n",
      "self-output: frozen Linear layer\n",
      "cross-query: r:  64 , input dim:  512 , output dim:  512\n",
      "cross-key: frozen Linear layer\n",
      "cross-value: r:  17 , input dim:  512 , output dim:  512\n",
      "cross-output: frozen Linear layer\n",
      "up: r:  20 , input dim:  512 , output dim:  856\n",
      "down: frozen Linear layer\n",
      "Layer: 3\n",
      "self-query: r:  36 , input dim:  512 , output dim:  512\n",
      "self-key: frozen Linear layer\n",
      "self-value: r:  15 , input dim:  512 , output dim:  512\n",
      "self-output: frozen Linear layer\n",
      "cross-query: r:  64 , input dim:  512 , output dim:  512\n",
      "cross-key: frozen Linear layer\n",
      "cross-value: r:  26 , input dim:  512 , output dim:  512\n",
      "cross-output: frozen Linear layer\n",
      "up: r:  36 , input dim:  512 , output dim:  1014\n",
      "down: frozen Linear layer\n",
      "Layer: 4\n",
      "self-query: r:  64 , input dim:  512 , output dim:  512\n",
      "self-key: frozen Linear layer\n",
      "self-value: r:  26 , input dim:  512 , output dim:  512\n",
      "self-output: frozen Linear layer\n",
      "cross-query: r:  64 , input dim:  512 , output dim:  512\n",
      "cross-key: frozen Linear layer\n",
      "cross-value: r:  26 , input dim:  512 , output dim:  512\n",
      "cross-output: frozen Linear layer\n",
      "up: r:  30 , input dim:  512 , output dim:  934\n",
      "down: frozen Linear layer\n",
      "Layer: 5\n",
      "self-query: r:  64 , input dim:  512 , output dim:  512\n",
      "self-key: frozen Linear layer\n",
      "self-value: r:  30 , input dim:  512 , output dim:  512\n",
      "self-output: frozen Linear layer\n",
      "cross-query: r:  64 , input dim:  512 , output dim:  512\n",
      "cross-key: frozen Linear layer\n",
      "cross-value: r:  19 , input dim:  512 , output dim:  512\n",
      "cross-output: frozen Linear layer\n",
      "up: r:  46 , input dim:  512 , output dim:  838\n",
      "down: frozen Linear layer\n",
      "Load weights from output/bubi\n"
     ]
    }
   ],
   "source": [
    "config, tokenizer, model = build_model(model_args, data_args, training_args, determined_model_path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaPT5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): AdaPT5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): AdaPT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): AdaPT5LayerSelfAttention(\n",
       "            (SelfAttention): AdaPT5Attention(\n",
       "              (q): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=55)\n",
       "                lora_B(in_features=55, out_features=512)\n",
       "              )\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=29)\n",
       "                lora_B(in_features=29, out_features=512)\n",
       "              )\n",
       "              (o): SelectLinear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): AdaPT5LayerFF(\n",
       "            (DenseReluDense): AdaPT5DenseActDense(\n",
       "              (wi): DistillLinear(in_features=512, out_features=1803, bias=False)\n",
       "                lora_A(in_features=512, out_features=58)\n",
       "                lora_B(in_features=58, out_features=1803)\n",
       "              )\n",
       "              (wo): SelectLinear(in_features=1803, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): AdaPT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): AdaPT5LayerSelfAttention(\n",
       "            (SelfAttention): AdaPT5Attention(\n",
       "              (q): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=55)\n",
       "                lora_B(in_features=55, out_features=512)\n",
       "              )\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=45)\n",
       "                lora_B(in_features=45, out_features=512)\n",
       "              )\n",
       "              (o): SelectLinear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): AdaPT5LayerFF(\n",
       "            (DenseReluDense): AdaPT5DenseActDense(\n",
       "              (wi): DistillLinear(in_features=512, out_features=1286, bias=False)\n",
       "                lora_A(in_features=512, out_features=26)\n",
       "                lora_B(in_features=26, out_features=1286)\n",
       "              )\n",
       "              (wo): SelectLinear(in_features=1286, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): AdaPT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): AdaPT5LayerSelfAttention(\n",
       "            (SelfAttention): AdaPT5Attention(\n",
       "              (q): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=64)\n",
       "                lora_B(in_features=64, out_features=512)\n",
       "              )\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=29)\n",
       "                lora_B(in_features=29, out_features=512)\n",
       "              )\n",
       "              (o): SelectLinear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): AdaPT5LayerFF(\n",
       "            (DenseReluDense): AdaPT5DenseActDense(\n",
       "              (wi): DistillLinear(in_features=512, out_features=883, bias=False)\n",
       "                lora_A(in_features=512, out_features=28)\n",
       "                lora_B(in_features=28, out_features=883)\n",
       "              )\n",
       "              (wo): SelectLinear(in_features=883, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): AdaPT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): AdaPT5LayerSelfAttention(\n",
       "            (SelfAttention): AdaPT5Attention(\n",
       "              (q): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=64)\n",
       "                lora_B(in_features=64, out_features=512)\n",
       "              )\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=45)\n",
       "                lora_B(in_features=45, out_features=512)\n",
       "              )\n",
       "              (o): SelectLinear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): AdaPT5LayerFF(\n",
       "            (DenseReluDense): AdaPT5DenseActDense(\n",
       "              (wi): DistillLinear(in_features=512, out_features=826, bias=False)\n",
       "                lora_A(in_features=512, out_features=15)\n",
       "                lora_B(in_features=15, out_features=826)\n",
       "              )\n",
       "              (wo): SelectLinear(in_features=826, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): AdaPT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): AdaPT5LayerSelfAttention(\n",
       "            (SelfAttention): AdaPT5Attention(\n",
       "              (q): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=64)\n",
       "                lora_B(in_features=64, out_features=512)\n",
       "              )\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=64)\n",
       "                lora_B(in_features=64, out_features=512)\n",
       "              )\n",
       "              (o): SelectLinear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): AdaPT5LayerFF(\n",
       "            (DenseReluDense): AdaPT5DenseActDense(\n",
       "              (wi): DistillLinear(in_features=512, out_features=708, bias=False)\n",
       "                lora_A(in_features=512, out_features=15)\n",
       "                lora_B(in_features=15, out_features=708)\n",
       "              )\n",
       "              (wo): SelectLinear(in_features=708, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): AdaPT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): AdaPT5LayerSelfAttention(\n",
       "            (SelfAttention): AdaPT5Attention(\n",
       "              (q): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=64)\n",
       "                lora_B(in_features=64, out_features=512)\n",
       "              )\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=26)\n",
       "                lora_B(in_features=26, out_features=512)\n",
       "              )\n",
       "              (o): SelectLinear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): AdaPT5LayerFF(\n",
       "            (DenseReluDense): AdaPT5DenseActDense(\n",
       "              (wi): DistillLinear(in_features=512, out_features=916, bias=False)\n",
       "                lora_A(in_features=512, out_features=28)\n",
       "                lora_B(in_features=28, out_features=916)\n",
       "              )\n",
       "              (wo): SelectLinear(in_features=916, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): AdaPT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): AdaPT5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): AdaPT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): AdaPT5LayerSelfAttention(\n",
       "            (SelfAttention): AdaPT5Attention(\n",
       "              (q): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=29)\n",
       "                lora_B(in_features=29, out_features=512)\n",
       "              )\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=45)\n",
       "                lora_B(in_features=45, out_features=512)\n",
       "              )\n",
       "              (o): SelectLinear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): AdaPT5LayerCrossAttention(\n",
       "            (EncDecAttention): AdaPT5Attention(\n",
       "              (q): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=64)\n",
       "                lora_B(in_features=64, out_features=512)\n",
       "              )\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=33)\n",
       "                lora_B(in_features=33, out_features=512)\n",
       "              )\n",
       "              (o): SelectLinear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): AdaPT5LayerFF(\n",
       "            (DenseReluDense): AdaPT5DenseActDense(\n",
       "              (wi): DistillLinear(in_features=512, out_features=1271, bias=False)\n",
       "                lora_A(in_features=512, out_features=35)\n",
       "                lora_B(in_features=35, out_features=1271)\n",
       "              )\n",
       "              (wo): SelectLinear(in_features=1271, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): AdaPT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): AdaPT5LayerSelfAttention(\n",
       "            (SelfAttention): AdaPT5Attention(\n",
       "              (q): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=27)\n",
       "                lora_B(in_features=27, out_features=512)\n",
       "              )\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=17)\n",
       "                lora_B(in_features=17, out_features=512)\n",
       "              )\n",
       "              (o): SelectLinear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): AdaPT5LayerCrossAttention(\n",
       "            (EncDecAttention): AdaPT5Attention(\n",
       "              (q): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=60)\n",
       "                lora_B(in_features=60, out_features=512)\n",
       "              )\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=17)\n",
       "                lora_B(in_features=17, out_features=512)\n",
       "              )\n",
       "              (o): SelectLinear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): AdaPT5LayerFF(\n",
       "            (DenseReluDense): AdaPT5DenseActDense(\n",
       "              (wi): DistillLinear(in_features=512, out_features=783, bias=False)\n",
       "                lora_A(in_features=512, out_features=23)\n",
       "                lora_B(in_features=23, out_features=783)\n",
       "              )\n",
       "              (wo): SelectLinear(in_features=783, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): AdaPT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): AdaPT5LayerSelfAttention(\n",
       "            (SelfAttention): AdaPT5Attention(\n",
       "              (q): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=38)\n",
       "                lora_B(in_features=38, out_features=512)\n",
       "              )\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=17)\n",
       "                lora_B(in_features=17, out_features=512)\n",
       "              )\n",
       "              (o): SelectLinear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): AdaPT5LayerCrossAttention(\n",
       "            (EncDecAttention): AdaPT5Attention(\n",
       "              (q): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=64)\n",
       "                lora_B(in_features=64, out_features=512)\n",
       "              )\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=17)\n",
       "                lora_B(in_features=17, out_features=512)\n",
       "              )\n",
       "              (o): SelectLinear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): AdaPT5LayerFF(\n",
       "            (DenseReluDense): AdaPT5DenseActDense(\n",
       "              (wi): DistillLinear(in_features=512, out_features=856, bias=False)\n",
       "                lora_A(in_features=512, out_features=20)\n",
       "                lora_B(in_features=20, out_features=856)\n",
       "              )\n",
       "              (wo): SelectLinear(in_features=856, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): AdaPT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): AdaPT5LayerSelfAttention(\n",
       "            (SelfAttention): AdaPT5Attention(\n",
       "              (q): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=36)\n",
       "                lora_B(in_features=36, out_features=512)\n",
       "              )\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=15)\n",
       "                lora_B(in_features=15, out_features=512)\n",
       "              )\n",
       "              (o): SelectLinear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): AdaPT5LayerCrossAttention(\n",
       "            (EncDecAttention): AdaPT5Attention(\n",
       "              (q): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=64)\n",
       "                lora_B(in_features=64, out_features=512)\n",
       "              )\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=26)\n",
       "                lora_B(in_features=26, out_features=512)\n",
       "              )\n",
       "              (o): SelectLinear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): AdaPT5LayerFF(\n",
       "            (DenseReluDense): AdaPT5DenseActDense(\n",
       "              (wi): DistillLinear(in_features=512, out_features=1014, bias=False)\n",
       "                lora_A(in_features=512, out_features=36)\n",
       "                lora_B(in_features=36, out_features=1014)\n",
       "              )\n",
       "              (wo): SelectLinear(in_features=1014, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): AdaPT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): AdaPT5LayerSelfAttention(\n",
       "            (SelfAttention): AdaPT5Attention(\n",
       "              (q): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=64)\n",
       "                lora_B(in_features=64, out_features=512)\n",
       "              )\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=26)\n",
       "                lora_B(in_features=26, out_features=512)\n",
       "              )\n",
       "              (o): SelectLinear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): AdaPT5LayerCrossAttention(\n",
       "            (EncDecAttention): AdaPT5Attention(\n",
       "              (q): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=64)\n",
       "                lora_B(in_features=64, out_features=512)\n",
       "              )\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=26)\n",
       "                lora_B(in_features=26, out_features=512)\n",
       "              )\n",
       "              (o): SelectLinear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): AdaPT5LayerFF(\n",
       "            (DenseReluDense): AdaPT5DenseActDense(\n",
       "              (wi): DistillLinear(in_features=512, out_features=934, bias=False)\n",
       "                lora_A(in_features=512, out_features=30)\n",
       "                lora_B(in_features=30, out_features=934)\n",
       "              )\n",
       "              (wo): SelectLinear(in_features=934, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): AdaPT5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): AdaPT5LayerSelfAttention(\n",
       "            (SelfAttention): AdaPT5Attention(\n",
       "              (q): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=64)\n",
       "                lora_B(in_features=64, out_features=512)\n",
       "              )\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=30)\n",
       "                lora_B(in_features=30, out_features=512)\n",
       "              )\n",
       "              (o): SelectLinear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): AdaPT5LayerCrossAttention(\n",
       "            (EncDecAttention): AdaPT5Attention(\n",
       "              (q): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=64)\n",
       "                lora_B(in_features=64, out_features=512)\n",
       "              )\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): DistillLinear(in_features=512, out_features=512, bias=False)\n",
       "                lora_A(in_features=512, out_features=19)\n",
       "                lora_B(in_features=19, out_features=512)\n",
       "              )\n",
       "              (o): SelectLinear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): AdaPT5LayerFF(\n",
       "            (DenseReluDense): AdaPT5DenseActDense(\n",
       "              (wi): DistillLinear(in_features=512, out_features=838, bias=False)\n",
       "                lora_A(in_features=512, out_features=46)\n",
       "                lora_B(in_features=46, out_features=838)\n",
       "              )\n",
       "              (wo): SelectLinear(in_features=838, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): AdaPT5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): AdaPT5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): DistillLinear(in_features=512, out_features=32128, bias=False)\n",
       "    lora_A(in_features=512, out_features=8)\n",
       "    lora_B(in_features=8, out_features=32128)\n",
       "  )\n",
       "  (layer_transformation): PruningLinear(in_features=512, out_features=512, bias=False)\n",
       "    lora_A(in_features=512, out_features=112)\n",
       "    lora_B(in_features=112, out_features=512)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'output/bubi',\n",
       " 'params': 68489548,\n",
       " 'params_trainable': 36136268,\n",
       " 'params_encoder': 30690456,\n",
       " 'params_decoder': 36900020,\n",
       " 'params_noembed': 52039500,\n",
       " 'mem_footprint': 273958192,\n",
       " 'dtype': torch.float32,\n",
       " 'device': device(type='cpu'),\n",
       " 'type': \"<class 'models.modeling_t5.AdaPT5ForConditionalGeneration'>\"}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_spec_dict = {\n",
    "    \"name\": model_path,\n",
    "    \"params\": model.num_parameters(),\n",
    "    \"params_trainable\": model.num_parameters(only_trainable=True),\n",
    "    \"params_encoder\": sum(p.numel() for p in model.encoder.parameters()),\n",
    "    \"params_decoder\": sum(p.numel() for p in model.decoder.parameters()),\n",
    "    \"params_noembed\": model.num_parameters(exclude_embeddings=True),\n",
    "    \"mem_footprint\": model.get_memory_footprint(),\n",
    "    \"dtype\": model.dtype,\n",
    "    \"device\": model.device,\n",
    "    \"type\": str(type(model)),\n",
    "}\n",
    "model_spec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 36136268\n",
      "all model parameters: 68489548\n",
      "percentage of trainable model parameters: 52.76%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gigatrue_preprocess_t5(batch, tokenizer, max_input_length, max_target_length, padding=\"max_length\", lang=\"en\"):\n",
    "    # add prefix to the input for t5\n",
    "    if lang == \"en\":\n",
    "        inputs = [\"summarize: \" + item for item in batch[\"article\"]]\n",
    "    elif lang == \"sk\":\n",
    "        inputs = [\"[SK] sumarizuj: \" + item for item in batch[\"article\"]]\n",
    " \n",
    "    # tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, padding=padding, truncation=True)\n",
    " \n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(text_target=batch[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
    " \n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    " \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing test dataset...\n",
      "Input shape: torch.Size([110])\n",
      "Labels shape: torch.Size([35])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 94406\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Tokenizing test dataset...\")\n",
    "\n",
    "ds_tokenized = dataset['test'].map(\n",
    "    gigatrue_preprocess_t5,\n",
    "    batched=True,\n",
    "    remove_columns=[\"orig_idx\"],\n",
    "    fn_kwargs=dict(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=\"max_length\",\n",
    "        lang='en',\n",
    "        max_input_length=data_args.max_input_length,\n",
    "        max_target_length=data_args.max_target_length\n",
    "    )\n",
    ")\n",
    "\n",
    "ds_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels', 'article', 'summary'])\n",
    "\n",
    "sample = ds_tokenized[0]\n",
    "print(\"Input shape:\", sample['input_ids'].shape)\n",
    "print(\"Labels shape:\", sample['labels'].shape)\n",
    "    \n",
    "ds_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries for test set...\n",
      "Generating summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/369 [00:07<46:20,  7.55s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Generate summaries for test set\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating summaries for test set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m generated_summaries \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_summaries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mds_tokenized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Print a sample comparison\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSample comparison:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[51], line 15\u001b[0m, in \u001b[0;36mgenerate_summaries\u001b[0;34m(model, tokenizer, dataset, device, batch_size)\u001b[0m\n\u001b[1;32m     12\u001b[0m current_batch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Generate summaries\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m summaries \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_target_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Decode the generated summaries\u001b[39;00m\n\u001b[1;32m     22\u001b[0m decoded_summaries \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(summaries, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniforge3/envs/apt/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/apt/lib/python3.10/site-packages/transformers/generation/utils.py:1602\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massisted_decoding(\n\u001b[1;32m   1586\u001b[0m         input_ids,\n\u001b[1;32m   1587\u001b[0m         assistant_model\u001b[38;5;241m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1598\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1599\u001b[0m     )\n\u001b[1;32m   1600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1616\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniforge3/envs/apt/lib/python3.10/site-packages/transformers/generation/utils.py:2450\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2447\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2449\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2450\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2453\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2455\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2457\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2458\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/apt/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/APT/models/modeling_t5.py:797\u001b[0m, in \u001b[0;36mAdaPT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, use_teacher, head_z, head_layer_z, intermediate_z, mlp_z, hidden_z, pass_mask)\u001b[0m\n\u001b[1;32m    794\u001b[0m     sequence_output \u001b[38;5;241m=\u001b[39m sequence_output \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dim \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head, lora\u001b[38;5;241m.\u001b[39mPruningLinear):\n\u001b[0;32m--> 797\u001b[0m     lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_teacher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_teacher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_retained_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretained_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m use_teacher:\n\u001b[1;32m    799\u001b[0m     lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(sequence_output)\n",
      "File \u001b[0;32m~/miniforge3/envs/apt/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def generate_summaries(model, tokenizer, dataset, device, batch_size=1):\n",
    "    # Initialize a list to store all summaries generated by the model\n",
    "    all_summaries = []\n",
    "    # Create a DataLoader for the dataset\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    current_batch = 0\n",
    "    \n",
    "    print(\"Generating summaries...\")\n",
    "    \n",
    "    # Iterate over the dataset\n",
    "    for batch in tqdm(dataloader):\n",
    "        current_batch += 1\n",
    "        \n",
    "        # Generate summaries\n",
    "        summaries = model.generate(\n",
    "            input_ids=batch['input_ids'].to(device),\n",
    "            attention_mask=batch['attention_mask'].to(device),\n",
    "            max_length=data_args.max_target_length,\n",
    "        )\n",
    "        \n",
    "        # Decode the generated summaries\n",
    "        decoded_summaries = tokenizer.batch_decode(summaries, skip_special_tokens=True)\n",
    "        \n",
    "        # Store the decoded summaries\n",
    "        all_summaries.extend(decoded_summaries)\n",
    "        \n",
    "    return all_summaries\n",
    "\n",
    "\n",
    "# Generate summaries for test set\n",
    "print(\"Generating summaries for test set...\")\n",
    "generated_summaries = generate_summaries(\n",
    "    model, \n",
    "    tokenizer,\n",
    "    ds_tokenized,\n",
    "    device,\n",
    "    batch_size=256,\n",
    ")\n",
    "\n",
    "# Print a sample comparison\n",
    "print(\"\\nSample comparison:\")\n",
    "print(\"Original:\", ds_tokenized[0]['summary'])\n",
    "print(\"Generated:\", generated_summaries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
